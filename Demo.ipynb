{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f80c146-a7d1-481c-a7a2-c327d19e58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gradio as gr\n",
    "from io import BytesIO\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68635cbd-e141-477d-9efe-74768e224b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jbodrenko/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer if not available\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e977d4f-9e5c-485c-9ec2-d3d29c83540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Labeling the dataset with questions and answers\n",
    "model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd1a77b-fe5c-4a6d-9fc0-387fe9dbaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "dataset_file = \"directive_dataset.json\"\n",
    "directive_pdf_url = \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32018L1972\"\n",
    "\n",
    "# Fetch and parse the directive text\n",
    "def fetch_directive_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the directive PDF\")\n",
    "    \n",
    "    pdf_file = BytesIO(response.content)\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "    # Fix hyphenation and normalize spaces\n",
    "    text = re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", text)  # Remove hyphenation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    text = re.sub(r'(\\d{2}\\.\\d{2}\\.\\d{4})', r'[\\1]', text)  # Wrap dates in square brackets\n",
    "    text = re.sub(r'\\bL\\s+\\d{3}/\\d{2}\\s+EN\\b', r'[L 321/98 EN]', text)  # Wrap references\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare dataset for Hugging Face tokenizers\n",
    "def prepare_huggingface_dataset(text, tokenizer, max_length=500, min_length=10):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_list = []\n",
    "    sent_lengths = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer(sentence, truncation=False, padding=False)\n",
    "        sentence_length = len(tokenized_sentence['input_ids'])  # Token length\n",
    "\n",
    "        if min_length < sentence_length <= max_length:\n",
    "            sentence_list.append({\"text\": sentence})\n",
    "            sent_lengths.append(sentence_length)\n",
    "            \n",
    "    sentence_lengths = {\n",
    "        \"avg_len\": np.mean(sent_lengths),\n",
    "        \"max_len\": max(sent_lengths),\n",
    "        \"min_len\": min(sent_lengths), \n",
    "        \"median_len\": np.median(sent_lengths),\n",
    "        \"std_len\": np.std(sent_lengths)\n",
    "    }\n",
    "    \n",
    "    print(f\"Median sentence length: {sentence_lengths['median_len']}\\nAvg sentence length: {sentence_lengths['avg_len']}\\nSentence length std: {sentence_lengths['std_len']}\\nMax sentence length: {sentence_lengths['max_len']}\\nMin sentence length: {sentence_lengths['min_len']}\")\n",
    "\n",
    "    return Dataset.from_list(sentence_list), sentence_lengths\n",
    "\n",
    "# Load or create dataset\n",
    "def load_or_create_dataset(tokenizer):\n",
    "    if os.path.exists(dataset_file):\n",
    "        print(\"Loading dataset from file...\")\n",
    "        with open(dataset_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset = Dataset.from_list(data[0])\n",
    "            sentence_lengths = data[1]\n",
    "            print(f\"Median sentence length: {sentence_lengths['median_len']}\\nAvg sentence length: {sentence_lengths['avg_len']}\\nSentence length std: {sentence_lengths['std_len']}\\nMax sentence length: {sentence_lengths['max_len']}\\nMin sentence length: {sentence_lengths['min_len']}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Fetching and processing directive...\")\n",
    "        directive_text = fetch_directive_pdf(directive_pdf_url)\n",
    "        dataset, sent_length = prepare_huggingface_dataset(directive_text, tokenizer)\n",
    "        with open(dataset_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([dataset.to_list(),sent_length], f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04718c70-3f88-4807-9b74-b1fef9f3ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file...\n",
      "Median sentence length: 56.0\n",
      "Avg sentence length: 66.8248807975726\n",
      "Sentence length std: 47.018184659819724\n",
      "Max sentence length: 487\n",
      "Min sentence length: 11\n"
     ]
    }
   ],
   "source": [
    "# Load or create dataset and show basic statistics of snippet length (in tokens)\n",
    "dataset = load_or_create_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc47582-0ea0-4ec0-9509-112c43fb410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2307\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dataset structure\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80650750-4e05-4e95-82a4-28688e6f40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(example, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a structured question-answer pair from input text, ensuring proper extraction.\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "\n",
    "    # Few-shot prompt for structured output\n",
    "    prompt = (\n",
    "        \"Generate a meaningful question-answer pair from the following directive text.\\n\"\n",
    "        f\"Text: {text}\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "\n",
    "    # Ensure padding token is correctly set\n",
    "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token  \n",
    "\n",
    "    # Tokenize input with proper padding and truncation\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'], # which tokens to ignore in input\n",
    "            max_length=512,  # truncation length\n",
    "            num_return_sequences=1, # produce single answer per input\n",
    "            pad_token_id=tokenizer.eos_token_id # what padding token was used\n",
    "        )\n",
    "\n",
    "    # Decode and clean output text\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Improved regex to extract the first valid Q&A pair\n",
    "    match = re.search(r'(?:Question|Q):\\s*(.*?)\\s*(?:Answer|A):\\s*(.*)', output_text, re.DOTALL)\n",
    "\n",
    "    # If result contains q-a pair in requested format\n",
    "    if match:\n",
    "        question = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "\n",
    "        # Clean up potential artifacts\n",
    "        question = re.sub(r'^(question_\\d+:|Solution:|\\s*<\\|assistant\\|>\\s*)', '', question, flags=re.IGNORECASE).strip()\n",
    "        answer = re.sub(r'^(answer_\\d+:)', '', answer, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        return {'question': question, 'answers': {'text': [answer]}}\n",
    "\n",
    "    # Otherwise assume annotation result is invalid\n",
    "    return {'question': None, 'answers': {'text': [None]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe062908-b81c-41aa-9bbe-25435ab2b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create subset of snippets for demo/testing purposes\n",
    "subset_path = 'directive_subset.json'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) #model name is defined with the tokenizer before\n",
    "def load_or_create_subset(dataset, subset_size, qa_generator, subset_path=subset_path, seed=None):\n",
    "    '''Wrapper to reduce repetitive annotation work.'''\n",
    "    \n",
    "    subset_path = subset_path.replace('.json', f'_{subset_size}.json')\n",
    "    \n",
    "    if os.path.exists(subset_path):\n",
    "        print(f\"Loading subset from {subset_path}\")\n",
    "        with open(subset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            subset = Dataset.from_list(json.load(f))            \n",
    "    else:\n",
    "        print(f\"Generating subset of {subset_size} snippets...\")\n",
    "        \n",
    "        # Randomly selecting subset of text snippets from the dataset (uniform prob.)\n",
    "        subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
    "        \n",
    "        # Generate a question-answer pair for each text snippet\n",
    "        subset = subset.map(lambda example: generate_qa(example, tokenizer, model))\n",
    "        \n",
    "        # Save to file\n",
    "        with open(subset_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(subset.to_list(), f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2e0ee7-a896-45e8-bfe9-19f8f9fa61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset from directive_subset_50.json\n"
     ]
    }
   ],
   "source": [
    "qa_subset = load_or_create_subset(dataset=dataset, subset_size=50, qa_generator=generate_qa, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490c97ae-9df5-4fea-a597-b8b640e4dab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset state after annotation\n",
    "qa_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c1a692c-da14-41a4-9276-85f3d4351c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only snippets where annotation results are valid\n",
    "valid_indices = [i for i,entry in enumerate(qa_subset) if entry['question'] is not None]\n",
    "qa_subset = qa_subset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060ca968-49e8-48e4-8214-500a8bb0dcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 21\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset state after removing snippets with invalid annotations\n",
    "qa_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b59314d-d4ef-472a-aee4-0a724fcadab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Train-test split without shuffling (need to be improved)\n",
    "valid_len = len(valid_indices)\n",
    "train_indices = round(0.8*valid_len)\n",
    "test_indices = train_indices\n",
    "train_set = qa_subset.select(range(train_indices))\n",
    "test_set = qa_subset.select(range(train_indices, valid_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8c333d-bdd0-44ff-bb4c-59be5dace2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set summary\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab638718-d6ff-4d2f-b17b-37d76cb976b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set summary\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6a0c4b-b1b2-418f-b6c6-43cfbe2b7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'That Annex should theref ore be deleted.', 'question': 'question for the text \"That Annex should theref ore be deleted.\"?', 'answers': {'text': ['does that annex should theref ore be deleted?\\nA:\\n<|assistant|>\\nDoes that Annex need to be deleted?']}}\n",
      "{'text': 'National regulato ry author ities should theref ore fully reflect any opinion submitted by BEREC in their measures imposing any oblig ation on an under taking or other wise resolving the dispute in such cases.', 'question': 'question regarding the directive?', 'answers': {'text': ['is it a directive?\\n<|assistant|>\\nIs the directive mentioned in the text?']}}\n"
     ]
    }
   ],
   "source": [
    "# Viewing the dataset contents to identify potential issues (\n",
    "print(test_set[0])\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b22a08-bafb-4369-98e0-523404c58d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to convert text to number and add some attributes required by the qa model to be fine-tuned\n",
    "model_checkpoint = \"google/flan-t5-base\"  # or \"t5-small\", \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function_no_context(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes question-answer pairs for training a generative model.\n",
    "    The model is trained to generate answers from the given questions.\n",
    "    \"\"\"\n",
    "    # Tokenize the questions as model input\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"question\"],  \n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Extract answers - empty strings need to be dealt with\n",
    "    answers_text = [ans[\"text\"][0] if ans[\"text\"] else \"\" for ans in examples[\"answers\"]]\n",
    "    \n",
    "    # Tokenize answers as labels\n",
    "    labels = tokenizer(\n",
    "        answers_text, \n",
    "        max_length=128, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eec87ab-c934-4044-8d7b-6247b4e111a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85e01d8fb6f4428a8c9798180fbd7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961dfa6fb8394c399dd34e20c640da55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the annotated datasets\n",
    "tokenized_train = train_set.map(preprocess_function_no_context, batched=True)\n",
    "tokenized_test = test_set.map(preprocess_function_no_context, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f4a5f5-3e43-41e9-9bd3-eaf09aadb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to fine-tune\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99beffd0-de56-496b-aa61-36958c794320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbodrenko/miniconda3/envs/nlp_tutorial/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d88b9141-69af-4970-930e-ac98f2e22563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting-up the training wrapper\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,  # If you split it earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21cd433-f6d8-404a-b852-f2cf0cf4eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 01:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>29.783703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>28.909153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>28.595795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=25.898856268988716, metrics={'train_runtime': 116.1908, 'train_samples_per_second': 0.439, 'train_steps_per_second': 0.077, 'total_flos': 34922624974848.0, 'train_loss': 25.898856268988716, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261557e8-8f87-4989-9ec0-af19ad281319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the trained model from the trainer\n",
    "model = trainer.model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1ae674e-9960-41ba-bcbe-70f6c13c1e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the results make sense\n",
    "def generate_answer(question):\n",
    "    '''Function called by the Gradio interface in the demo.'''\n",
    "    # Format input for T5 (T5 expects a 'question:' prefix)\n",
    "    input_text = f\"question: {question}\"\n",
    "    \n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(inputs.input_ids, max_length=100)\n",
    "    \n",
    "    # Decode and print answer\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_answer\n",
    "\n",
    "# Running gradio demo interface\n",
    "textbox = gr.Textbox(label=\"Type your question here:\", placeholder=\"What is the directive about?\", lines=10)\n",
    "\n",
    "gr.Interface(fn=generate_answer, inputs=textbox, outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
