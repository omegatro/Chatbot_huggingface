{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f80c146-a7d1-481c-a7a2-c327d19e58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gradio as gr\n",
    "from io import BytesIO\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68635cbd-e141-477d-9efe-74768e224b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jbodrenko/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer if not available\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e977d4f-9e5c-485c-9ec2-d3d29c83540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Labeling the dataset with questions and answers\n",
    "model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd1a77b-fe5c-4a6d-9fc0-387fe9dbaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "dataset_file = \"directive_dataset.json\"\n",
    "directive_pdf_url = \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32018L1972\"\n",
    "\n",
    "# Fetch and parse the directive text\n",
    "def fetch_directive_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the directive PDF\")\n",
    "    \n",
    "    pdf_file = BytesIO(response.content)\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "    # Fix hyphenation and normalize spaces\n",
    "    text = re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", text)  # Remove hyphenation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    text = re.sub(r'(\\d{2}\\.\\d{2}\\.\\d{4})', r'[\\1]', text)  # Wrap dates in square brackets\n",
    "    text = re.sub(r'\\bL\\s+\\d{3}/\\d{2}\\s+EN\\b', r'[L 321/98 EN]', text)  # Wrap references\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare dataset for Hugging Face tokenizers\n",
    "def prepare_huggingface_dataset(text, tokenizer, max_length=500, min_length=10):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_list = []\n",
    "    sent_lengths = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer(sentence, truncation=False, padding=False)\n",
    "        sentence_length = len(tokenized_sentence['input_ids'])  # Token length\n",
    "\n",
    "        if min_length < sentence_length <= max_length:\n",
    "            sentence_list.append({\"text\": sentence})\n",
    "            sent_lengths.append(sentence_length)\n",
    "            \n",
    "    sentence_lengths = {\n",
    "        \"avg_len\": np.mean(sent_lengths),\n",
    "        \"max_len\": max(sent_lengths),\n",
    "        \"min_len\": min(sent_lengths), \n",
    "        \"median_len\": np.median(sent_lengths),\n",
    "        \"std_len\": np.std(sent_lengths)\n",
    "    }\n",
    "    \n",
    "    print(f\"Median sentence length: {sentence_lengths['median_len']}\\nAvg sentence length: {sentence_lengths['avg_len']}\\nSentence length std: {sentence_lengths['std_len']}\\nMax sentence length: {sentence_lengths['max_len']}\\nMin sentence length: {sentence_lengths['min_len']}\")\n",
    "\n",
    "    return Dataset.from_list(sentence_list), sentence_lengths\n",
    "\n",
    "# Load or create dataset\n",
    "def load_or_create_dataset(tokenizer):\n",
    "    if os.path.exists(dataset_file):\n",
    "        print(\"Loading dataset from file...\")\n",
    "        with open(dataset_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset = Dataset.from_list(data[0])\n",
    "            sentence_lengths = data[1]\n",
    "            print(f\"Median sentence length: {sentence_lengths['median_len']}\\nAvg sentence length: {sentence_lengths['avg_len']}\\nSentence length std: {sentence_lengths['std_len']}\\nMax sentence length: {sentence_lengths['max_len']}\\nMin sentence length: {sentence_lengths['min_len']}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Fetching and processing directive...\")\n",
    "        directive_text = fetch_directive_pdf(directive_pdf_url)\n",
    "        dataset, sent_length = prepare_huggingface_dataset(directive_text, tokenizer)\n",
    "        with open(dataset_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([dataset.to_list(),sent_length], f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04718c70-3f88-4807-9b74-b1fef9f3ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file...\n",
      "Median sentence length: 56.0\n",
      "Avg sentence length: 66.8248807975726\n",
      "Sentence length std: 47.018184659819724\n",
      "Max sentence length: 487\n",
      "Min sentence length: 11\n"
     ]
    }
   ],
   "source": [
    "# Load or create dataset and show basic statistics of snippet length (in tokens)\n",
    "dataset = load_or_create_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc47582-0ea0-4ec0-9509-112c43fb410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2307\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dataset structure\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80650750-4e05-4e95-82a4-28688e6f40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(example, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a structured question-answer pair from input text, ensuring proper extraction.\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "\n",
    "    # Few-shot prompt for structured output\n",
    "    prompt = (\n",
    "        \"Generate a meaningful question-answer pair from the following directive text.\\n\"\n",
    "        f\"Text: {text}\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "\n",
    "    # Ensure padding token is correctly set\n",
    "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token  \n",
    "\n",
    "    # Tokenize input with proper padding and truncation\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'], # which tokens to ignore in input\n",
    "            max_length=512,  # truncation length\n",
    "            num_return_sequences=1, # produce single answer per input\n",
    "            pad_token_id=tokenizer.eos_token_id # what padding token was used\n",
    "        )\n",
    "\n",
    "    # Decode and clean output text\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Improved regex to extract the first valid Q&A pair\n",
    "    match = re.search(r'(?:Question|Q):\\s*(.*?)\\s*(?:Answer|A):\\s*(.*)', output_text, re.DOTALL)\n",
    "\n",
    "    # If result contains q-a pair in requested format\n",
    "    if match:\n",
    "        question = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "\n",
    "        # Clean up potential artifacts\n",
    "        question = re.sub(r'^(question_\\d+:|Solution:|\\s*<\\|assistant\\|>\\s*)', '', question, flags=re.IGNORECASE).strip()\n",
    "        answer = re.sub(r'^(answer_\\d+:)', '', answer, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        return {'question': question, 'answers': {'text': [answer]}}\n",
    "\n",
    "    # Otherwise assume annotation result is invalid\n",
    "    return {'question': None, 'answers': {'text': [None]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe062908-b81c-41aa-9bbe-25435ab2b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create subset of snippets for demo/testing purposes\n",
    "subset_path = 'directive_subset.json'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) #model name is defined with the tokenizer before\n",
    "def load_or_create_subset(dataset, subset_size, qa_generator, subset_path=subset_path, seed=None):\n",
    "    '''Wrapper to reduce repetitive annotation work.'''\n",
    "    \n",
    "    subset_path = subset_path.replace('.json', f'_{subset_size}.json')\n",
    "    \n",
    "    if os.path.exists(subset_path):\n",
    "        print(f\"Loading subset from {subset_path}\")\n",
    "        with open(subset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            subset = Dataset.from_list(json.load(f))            \n",
    "    else:\n",
    "        print(f\"Generating subset of {subset_size} snippets...\")\n",
    "        \n",
    "        # Randomly selecting subset of text snippets from the dataset (uniform prob.)\n",
    "        subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
    "        \n",
    "        # Generate a question-answer pair for each text snippet\n",
    "        subset = subset.map(lambda example: generate_qa(example, tokenizer, model))\n",
    "        \n",
    "        # Save to file\n",
    "        with open(subset_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(subset.to_list(), f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2e0ee7-a896-45e8-bfe9-19f8f9fa61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset from directive_subset_50.json\n"
     ]
    }
   ],
   "source": [
    "subset_size = 50\n",
    "qa_subset = load_or_create_subset(dataset=dataset, subset_size=subset_size, qa_generator=generate_qa, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490c97ae-9df5-4fea-a597-b8b640e4dab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset state after annotation\n",
    "qa_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c1a692c-da14-41a4-9276-85f3d4351c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only snippets where annotation results are valid\n",
    "valid_indices = [i for i,entry in enumerate(qa_subset) if entry['question'] is not None]\n",
    "qa_subset = qa_subset.select(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060ca968-49e8-48e4-8214-500a8bb0dcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset state after removing snippets with invalid annotations\n",
    "qa_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b59314d-d4ef-472a-aee4-0a724fcadab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Train-test split without shuffling (need to be improved)\n",
    "valid_len = len(valid_indices)\n",
    "train_indices = round(0.8*valid_len)\n",
    "test_indices = train_indices\n",
    "train_set = qa_subset.select(range(train_indices))\n",
    "test_set = qa_subset.select(range(train_indices, valid_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8c333d-bdd0-44ff-bb4c-59be5dace2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set summary\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab638718-d6ff-4d2f-b17b-37d76cb976b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'question', 'answers'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set summary\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6a0c4b-b1b2-418f-b6c6-43cfbe2b7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This would be the case for exam ple if network operators were to restr ict unreasonably end-user choice for access to internet portals and services.', 'question': 'What would be the case for example if network operators restricted unreasonably end-user choice for access to internet portals and services?', 'answers': {'text': ['network operators']}}\n",
      "{'text': 'Those barriers should be reduced by the applicability of the same rules ensur ing a high common level of prot ection across the Union.', 'question': 'what is the main idea of the directive?', 'answers': {'text': ['those barriers should be reduced by the applicability of the same rules ensur ing a high common level of prot ection across the Union.']}}\n"
     ]
    }
   ],
   "source": [
    "# Viewing the dataset contents to identify potential issues (\n",
    "print(test_set[0])\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b22a08-bafb-4369-98e0-523404c58d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to convert text to number and add some attributes required by the qa model to be fine-tuned\n",
    "model_checkpoint = \"google/flan-t5-base\"  # or \"t5-small\", \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function_no_context(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes question-answer pairs for training a generative model.\n",
    "    The model is trained to generate answers from the given questions.\n",
    "    \"\"\"\n",
    "    # Combine directive text and question\n",
    "    inputs = [f\"Context: {t} Question: {q}\" for t, q in zip(examples[\"text\"], examples[\"question\"])]\n",
    "\n",
    "    # Tokenize the input (context + question)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,  \n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Extract answers - empty strings need to be dealt with\n",
    "    answers_text = [ans[\"text\"][0] if ans[\"text\"] else \"\" for ans in examples[\"answers\"]]\n",
    "    \n",
    "    # Tokenize answers as labels\n",
    "    labels = tokenizer(\n",
    "        answers_text, \n",
    "        max_length=256, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eec87ab-c934-4044-8d7b-6247b4e111a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b26add870b489cb9c8c653fcad5f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc589103f4b74fcfa7fcac2c44bc17e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the annotated datasets\n",
    "tokenized_train = train_set.map(preprocess_function_no_context, batched=True)\n",
    "tokenized_test = test_set.map(preprocess_function_no_context, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f4a5f5-3e43-41e9-9bd3-eaf09aadb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to fine-tune\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99beffd0-de56-496b-aa61-36958c794320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbodrenko/miniconda3/envs/nlp_tutorial/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d88b9141-69af-4970-930e-ac98f2e22563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting-up the training wrapper\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,  # If you split it earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21cd433-f6d8-404a-b852-f2cf0cf4eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 02:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.994389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.347374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.388622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "save_path = f\"{subset_size}_snippet_{model_checkpoint.replace('/','_')}\"\n",
    "if not os.path.isfile(save_path):\n",
    "    # Training\n",
    "    trainer.train()\n",
    "\n",
    "    # Saving the model to a file\n",
    "    trainer.model.save_pretrained(f'{save_path}.model')\n",
    "    \n",
    "    # Retrieving the trained model from the trainer\n",
    "    model = trainer.model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "else:\n",
    "    # Load model from saved results & tokenizer for its base model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(f'{save_path}.model')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4746d8a0-fb7d-410e-bb95-02681e823446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Efficient & fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "131d316d-595e-4f7f-afe5-16578e0477f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text snippets into embeddings to find most relevant context based on the question\n",
    "document_embeddings = embedder.encode(qa_subset['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4c9633-2ca2-45c0-a7d2-7778c7067cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: How ever , the national regulator y author ities should still be able to imp ose obliga tions and conditions on under takings that control access to end-users in order to maintain access and comp etition in that market.\n"
     ]
    }
   ],
   "source": [
    "# Use cosine similarity on document and question embeddings to find the most relevant context snippet \n",
    "def retrieve_context(question, documents, document_embeddings, embedder):\n",
    "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarities = cosine_similarity(question_embedding, document_embeddings)\n",
    "    \n",
    "    # Retrieve the most similar passage\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    return documents[best_idx]\n",
    "\n",
    "question = \"What should the national regulator y author ities do to maintain access and competition in the market?\"\n",
    "retrieved_context = retrieve_context(question, qa_subset['text'], document_embeddings, embedder)\n",
    "print(\"Retrieved Context:\", retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b76c465-f34a-491d-aeec-7b7e0f73dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: imp ose obliga tions and conditions on under takings\n"
     ]
    }
   ],
   "source": [
    "# Combine snippet and question and use the model to produce the answer\n",
    "def generate_answer(question, model, tokenizer, context):\n",
    "    input_text = f\"Question: {question} Context: {context}\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    output = model.generate(**inputs, max_length=200)\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example\n",
    "answer = generate_answer(question, model, tokenizer, retrieved_context)\n",
    "print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1ae674e-9960-41ba-bcbe-70f6c13c1e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_from_model(question, model, tokenizer, context):\n",
    "    # Incorporate\n",
    "    input_text = f\"Question: {question} Context: {context}\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    output = model.generate(**inputs, max_length=200)\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_answer(question):\n",
    "    # Get relevant context\n",
    "    retrieved_context = retrieve_context(question, qa_subset['text'], document_embeddings, embedder)\n",
    "\n",
    "    # Produce the answer with fine-tuned model using the question and context as input\n",
    "    return generate_answer_from_model(question, model, tokenizer, retrieved_context)\n",
    "\n",
    "# Running gradio demo interface\n",
    "textbox = gr.Textbox(label=\"Type your question here:\", placeholder=\"What is the directive about?\", lines=10)\n",
    "\n",
    "gr.Interface(fn=generate_answer, inputs=textbox, outputs=\"text\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571fea7-d90b-4866-9ef2-d85c0c02dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
